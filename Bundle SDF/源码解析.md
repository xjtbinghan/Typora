[TOC]



# 入口——run_custom.py

## step 1 track和nerf配置

## step 2 初始化 BundleSdf实例和YcbineoatReader 实例

​		初始化BundleSdf实例时，做了三件事：

​		第一，把track和nerf的配置文件读取为*self*.cfg_track和*self*.cfg_nerf。	

​		第二，创建了gui和nerf进程，使用*multiprocessing.Manager*管理进程间的共享数据，其中GUI的共享数据使用gui_dict储存，nerf的共享数据使用p_dict 和 kf_to_nerf_list储存。

​		第三，初始化了my_cpp.Bundle 和LoftrRunner，其最终my_cpp.Bundle 用于调用c++程序，LoftrRunner用于调用Loftr做特征匹配。

## step 3 跟踪每帧——Track.run()

## step 4 跟踪结束——tracker.on_finish()

## step 5 nerf环节——run_one_video_global_nerf



# 跟踪主函数——tracker.run

## 0 成员变量列表

### Bundlesdf类

​		说明：下列的self.代表Bundlesdf.  因为是在Bundlesdf类中的属性(成员变量)或方法(函数)，所以在Bundlesdf中使用self

| 变量                   | 类型                                    | 意义                                                         |
| ---------------------- | --------------------------------------- | ------------------------------------------------------------ |
| *self*.bundler._frames | std::map< int,std::shared_ptr< Frame >> | 储存索引和帧对象的字典                                       |
| ref_frame              | std::shared_ptr< Frame >                | 参考帧（代码中使用上一帧作为参考帧）                         |
| min_match_with_ref     | int                                     | 和参考帧的最少匹配数目                                       |
| *self*.cfg_track       |                                         | 经过修改的cfg_track_dir的文件读取类型                        |
| frame                  | 是my_cpp的Frame类                       | 经过step1 构造后的当前帧                                     |
| imgs                   |                                         |                                                              |
| tfs                    |                                         |                                                              |
| query_pairs            | std::vector<FramePair>                  | 用于存放帧对的列表，列表中每个元素是一对帧                   |
| corres                 |                                         |                                                              |
| self.p_dict            | *multiprocessing.Manager*的字典结构     | nerf环节所共享的字典数据，用于储存？                         |
| self.kf_to_nerf_list   | *multiprocessing.Manager*的列表结构     | nerf环节所共享的列表数据，用于临时储存待传入nerf进程的关键帧数据，包括该关键帧的_rgb _depth 和mask等信息。在传入nerf后会清空该帧。 |
| cam_in_obs             | list                                    | 储存关键帧位姿的列表                                         |
|                        |                                         |                                                              |

### Bundler类

​		Bundler是储存帧类，特征匹配类等的类，通过Bundlesdf.bundler调用。

| 变量        | 类型                                    | 意义                                                   |
| ----------- | --------------------------------------- | ------------------------------------------------------ |
| _frames     | std::map< int,std::shared_ptr< Frame >> | 储存索引和帧对象的字典，只有跟踪成功了才会放到该字典中 |
| _keyframes  | std::deque< std::shared_ptr< Frame>>    | 关键帧队列                                             |
| _newframe   |                                         | 当前帧                                                 |
| _firstframe |                                         | 第一帧                                                 |
| _fm         | std::shared_ptr< GluNet>                | 特征匹配类FeatureManager                               |
|             |                                         |                                                        |

### FeatureManager类

| 变量           | 类型                                                         | 意义                                                         |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| *_raw_matches* | std::map<FramePair, Eigen::Matrix<uint16_t, Eigen::Dynamic, Eigen::Dynamic>> | 字典的键是帧对，值是储存他们匹配点对xy坐标的（N，4）矩阵。在每一次特征匹配后会注册。也用于检查是否已经处理过该帧对 |
| imgs           | std::vector< cv::Mat>                                        | 向量：长度为2m，m表示有m个帧对，imgs表示用于做Loftr特征匹配的2m个图像Mat的向量 |
| tfs            | std::vector< Eigen::Matrix3f>                                | 向量：长度为2m，m表示有m个帧对，tfs表示用于匹配之前图像预处理的每个帧对的两个图像的旋转矩阵R |
| query_pairs    | std::vector< FramePair>                                      | 向量，长度为m ，表示用于做匹配的帧对。                       |
| out_side       | int                                                          | 投入Loftr之前resize的图像尺寸                                |
| _matches       | std::map<FramePair, std::vector<Correspondence >>            | 键表示每一个匹配的帧对，值代表该储存筛选为内点的匹配关系的Correspondence类对象corres。Loftr匹配之后会注册 |
| Correspondence | 用来储存两帧间每一个内点匹配点对信息的类                     | 类内变量包括 _uA, _uB _vA, _vB, _ptA_cam, _ptB_cam _isinlier, _confidence  。表示该匹配点对应的帧A的UV坐标，帧B的UV坐标  ，帧A帧B的点云坐标和内外点Bool标签，以及置信度(统一为1 ，因为已经是内点) |

​    

## step 1 构造Frame，c++数据类型

​		每一帧的数据使用Frmae类表示，他是c++的数据类型，**详见my_cpp.Frame**

### 1.1 初始化Frame类的成员变量——Frmae::Frame    

​		使用入口函数中传输的参数初始化成员变量，成员变量如以下，其中_color和 _depth由于是使用==python的opencv读取的，是numpy数据格式==，在c++程序中不能识别，所以要==转化为c++代码的cv::Mat数据格式==。其他的都直接赋值就行。

#### Frame类的成员变量列表

| 变量           | 类型                         | 意义                                                         |
| -------------- | ---------------------------- | ------------------------------------------------------------ |
| _status        | Status枚举类型               | 用于表示系统状态                                             |
| _color         | cv::Mat< cv::Vec3b >         | RGB数据（CPU内存中的）                                       |
| _color_raw     | cv::Mat                      | 用_color.clone的，不知道要做什么                             |
| _depth         | cv::Mat                      | Depth数据（CPU内存中的）                                     |
| _depth_raw     | cv::Mat                      | 用_depth.clone的，不知道要做什么                             |
| _depth_sim     | cv::Mat                      | 用_depth.clone的，不知道要做什么                             |
| _roi           | Eigen::Vector4f              | 表示Mask掩码不为0的包围框角点*umin,umax,vmin,vmax*           |
| _id            | int                          | 帧的计数（从0开始）                                          |
| _id_str        | std::string                  | 每帧的文件名（无后缀）                                       |
| _K             | Eigen::Matrix3f              | 内参矩阵                                                     |
| yml            | std::shared_ptr<YAML::Node > | cfg_track_dir，用于储存track配置参数的文件变量               |
| _pose_in_model | Eigen::Matrix4f              | 单位变换矩阵，表示初始位姿。                                 |
| _cloud         | PointCloudRGBNormal::Ptr     | 点云对象指针（CPU内存中的）                                  |
| _cloud_down    | PointCloudRGBNormal::Ptr     |                                                              |
| _normal_map    | cv::Mat< cv::Vec3f >         | 点云法线向量（CPU内存中的）                                  |
| _depth_gpu     | float *                      | 用于放在GPU上运算的depth指针。                               |
| _normal_gpu    | float4 *                     | 用于放在GPU上运算的法线向量指针。                            |
| _color_gpu     | uchar4 *                     |                                                              |
| depth_tmp_gpu  |                              | 1.2.4中给深度做侵蚀和高斯滤波预处理时用于在GPU上计算的临时变量 |
| _ref_frame_id  | int                          | 表示参考帧的ID，由Bundlesdf的ref_frame传入                   |
|                |                              |                                                              |

### 1.2 利用GPU对深度数据和点云预处理—— Frame::init() 

处理深度数据，初始化和筛选点云，并得到包围盒  

#### 1.2.1 初始位姿的旋转矩阵规范化

​		把初始位姿_pose_in_model中的旋转矩阵R规范化，使其行列式为1，R的列向量彼此正交。Utils::normalizeRotationMatrix ()函数。==规范个der，本来就是单位变换，还规范==

#### 1.2.2 建立空的点云地图 

​		使用boost::make_shared创建了一个动态分配内存的指针，让指针指向一个pcl点云类型的空对象，< pcl::PointCloud<pcl::PointXYZRGBNormal >>() ,这个数据结构表示每个点包含三维坐标（X、Y、Z）、颜色信息（RGB）、以及法向量信息（Normal），表示初始的还未添加地图点的点云地图。

#### 1.2.3 在GPU上新建_depth_gpu、 _normal_gpu 和 _color_gpu数据的内存

​		首先：使用cudaMalloc()，在GPU上新建了& _depth_gpu、& _normal_gpu和 & _color_gpu三个内存，分别用于存放深度、RGB和法向量数据。只不过数据类型不是矩阵而是展平之后的数组。

​		然后：使用cudaMemset()，把这些数据的元素值先设为0，用于后续的赋值。 

#### 1.2.4 把深度数据放在_depth_gpu用GPU处理(侵蚀和高斯滤波) 再放回CPU内存的 _depth中

​		首先：使用Frame::updateDepthGPU()把深度数据 _depth拷贝到 _depth_gpu中，把深度数据放到GPU上，并把数据格式改为展平之后的Mat（1，n_pixels）。

​		然后：使用Frame::processDepth()把深度数据做侵蚀和高斯滤波处理去噪，处理完之后再使用Frame::updateDepthCPU()，把_depth_gpu拷贝回 _depth中，把深度数据重新放到CPU上。==仿真的数据深度特别准，没有噪声，所以就不进行侵蚀和滤波了==

#### 1.2.5 恢复点云_cloud，并给其添加xyz，rgb和法线向量数据

​		首先：使用处理之后的深度数据_depth_gpu恢复点云（xyz_map_gpu），然后计算点云的法线向量。基于点云法线向量使用CUDAImageUtil::filterDepthSmoothedEdges以平滑边缘区域的深度。

​		然后：使用平滑之后深度数据 _depth_gpu再次重新恢复点云（xyz_map_gpu），顺便把平滑之后的深度数据再同步更新给CPU上的 _depth。借助xyz_map_gpu、 _normal_gpu和 _color 把xyz、rgb和法线向量数据信息填充到 _cloud中。 ==不平滑深度数据了==

==初始化点云是最费时间的，对输入数据下采样之后会==

#### 1.2.6 第一帧的时候，计算点云的包围盒

==可能用于可视化图像的外3D包围盒，不做==

​		使用pcl::PassThrough< PointT >filter筛选点云中深度在0.1m～1m范围内的点， 然后使用pcl::getMinMax3D来得到包围盒的上界和下界。

ps：_cloud没有改变，筛选之后的点云保存到了临时变量cloud_world中，只是为了计算包围盒；（这个筛选0.1m～1m是针对数据集而设计的，配置文件的参数需要根据数据集而调整） 

### 1.3 初始化Mask—my_cpp.cvMat(mask)

​		把Mask从 python中的numpy数据格式转化为cv::mat格式

## step 2 跟踪frame —— process_new_frame()

​		跟踪帧主要的工作是：（特征匹配+RANSAC ICP+重定位+局部BA+插入关键帧）



### 2.1 设置第一帧  或  参考帧和初始位姿(参考帧位姿)	

​		如果不是第一帧：上一个跟踪成功的帧作为参考帧，初始位姿使用参考帧的位姿。==这里初始位姿是用来在特征匹配之前处理帧对的，后面计算位姿是使用RANSAC+ICP并没有使用非线性优化==

​		如果是第一帧    ：把当前帧设为*self*.bundler._firstframe。

疑问：没有使用恒速模型，个人觉得添加后效果会更好  

回答：之后计算位姿是使用RANSAC+线性求解ICP，不需要初始位姿，初始位姿是用来做特征匹配之前的图像线性变换的。

### 2.2 基于掩码，更新_roi，更新 _color、 _depth、 _gray和点云 _cloud

​		首先：找到Mask ！= 0位置的包围框，用来更新根感兴趣区域 _roi；

​		其次：Mask为0的位置uv（下图非绿色部分）对应的图像和点云属性全部更新为0，包括_color、 _depth、 _gray矩阵和pcl点云 _cloud的xyz，法线向量属性更新为0。

​		==详见frame.invalidatePixelsByMask(frame._fg_mask)==

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/IMG_20231022_102714.jpg" alt="IMG_20231022_102714" style="zoom: 10%;" />

### 2.3 对第一帧的点云文件保存，设置初始平移t

​		首先：对于在Ｍask内深度>0.1m的有效点云，拷贝给cloud变量中，以PLY格式保存在output路径中
​		然后：对有效点云进行离群点剔除，更新剔除后的点云cloud，以PLY格式保存在output路径中。
​		最后：计算剔除后的有效点云的最大和最小xyz坐标，把(max_xyz+min_xyz)/2作为位姿_pose_in_model的平移部分t。==详见frame.setNewInitCoordinate()==

### 2.4 判断该帧是否合格，条件1：

​		如果该帧有效Mask区域(!= 0)的像素数量 不超过100个，则认为该帧无效，设置系统状态_statues为FAIL，忘记该帧，然后Return；，==self.bundler.forgetFrame(frame)==

### 2.5 点云去噪处理

​		如果配置里点云去噪为Ture，那么对点云去噪处理，==详见frame.pointCloudDenoise()==

### 2.6 判断该帧是否合格，条件2:

​		如果该帧有效Mask区域(!= 0)大于0.1的像素数量 <初始帧的1/40，则认为该帧无效，设置系统状态_statues为FAIL，然后Return；。==这里源码写代码重复计算了n_valid_first==，导致每一帧都需要重新计算第一帧数量，可以加个判断语句，只需要在第一帧计算。

### 2.7 第一帧设为关键帧，记录后返回

​		如果是第一帧，那么把第一帧设为关键帧，然后把当前帧添加到*bundler._frames*字典中记录，然后Return；。详见==*self*.bundler.checkAndAddKeyframe(frame)==。

### 2.8 特征匹配、RANSAC+ICP单帧位姿估计——find_corres()

#### 2.8.1 处理帧对，图像坐标透视变换——GluNet::getProcessedImagePairs

​		对于当前帧和参考帧构造成的帧对（对应着Feature.Manager中的FramePair格式）使用==bundler._fm.getProcessedImagePairs()==函数对其处理，得到处理后的imgs，tfs和query_pairs。其中imgs是长度为2的向量，包括处理之后的用于匹配的imgA, imgB。tfs是长度为2的向量，包括tfsA 和 tfsB，分别表示从帧A到B的坐标旋转矩阵RA 和从帧B到帧A的旋转矩阵RB。query_pairs是长度为1 的向量，向量中有一个FramePair类型的帧对，分别储存这FrameA 和 FrameB。

​		在bundler. _fm.getProcessedImagePairs()函数处理的过程中，使用了processImagePair函数，将输入frame的图像进行旋转、平移和缩放，旨在得到更准确的匹配结果。具体的操作是：基于图像的 _roi区域，让两个图像的mask区域内的有效部分都用一个线性的透视变换（旋转平移缩放）到一个400 ✖ 400尺寸的框内。

​		==为什么要这样做？==这里不太明白，为什么要这样转换，这样一来，目标在地图中的坐标不就乱了吗，难道直接对原图裁减使用Loftr不行吗，使用大的roi窗口作为标准，保留两个图像在原来图像中的uv坐标，这样不行吗，为什么还要使用什么轴角z轴的旋转，不懂？

​		==答：== 透视变换是一个线性变换，这意味着变换之后的每一个坐标都可以反映射到变换之前的原始坐标。mask区域对于整个图像来说，只是一小部分，可能会由于尺度变小而影响Loftr这类特征提取算法的定位精度。所以通过这种方式，让他的像素点放大，提取之后在线性变换回去，这样在新图像上就可以避免因为尺度过小而导致的偏差过大，可以降低误差。

   <img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/mmexport1698068641635.png" alt="mmexport1698068641635" style="zoom: 33%;" />

#### 2.8.2 使用Loftr对imgs中变换后的两个图像做特征匹配——*self*.loftr.predict()

​		使用Loftr找到特征匹配，把所有对图像的匹配结果储存在**corres内**（假设传入Loftr.predict的有m对图像），那么corres是一个维度为（m，N，5）的数组。 

​		其中m是channel表示图像对的数目；N是行表示这两个图像产生的匹配点对数目；5是列，每行包含5个元素，这些元素表示一个匹配的特征点对，前两个元素是第一个特征点的坐标，接下来两个元素是第二个特征点的坐标，最后一个元素是匹配的置信度或相关性分数。(X0, Y0, X1, Y1, 置信度) 

#### 2.8.3 图像坐标反变换后注册在*self*.bundler._fm._raw_matches中等待筛选

​		将匹配后的坐标变换回2.8.1处理之前（loftr得到的匹配坐标 是转化之后的图像，这里匹配了之后再转化回来），然后把小数点坐标四舍五入之后，注册到*self*.bundler._fm._raw_matches中，作为这对帧的匹配结果。corres中的置信度没有注册进去，作者在后面可能使用了所有的匹配点还做了筛选，筛选通过的作为内点，置信度统一为1 ，所以这里就没有用置信度。

#### 2.8.4  匹配数量太少，该帧状态设为FAIL

​		如果匹配到的数量，少于最小阈值，把bundler._fm._raw_matches中的匹配记录注销，把该帧状态*self*.bundler._newframe._status设为FAIL，然后返回到 Step2 流程，根据当前的环节，执行2.9或2.14。在2.9和2.14中如果当前帧frame的状态时FAIL时，会使用*self*.bundler.forgetFrame(frame)，把该帧遗忘。

#### 2.8.5 筛选匹配点对，将内点注册在*self*.bundler. _fm. _matches中

​		**首先：**把帧对送入bundler._fm.GluNet::rawMatchesToCorres()函数，把帧对得到的匹配对坐标一一拿出来，将每个匹配点对通过FM类的makeCorrespondence()函数筛选内外点，在筛选的过程中有以下三个条件：

​		条件一：特征点必须在图像边界内；

​		条件二：特征点的点云深度必须>0.1m；==（不鲁棒，对于不同的数据集需要手动调整）==

​		条件三：世界坐标系下的点云距离和法向量点积在阈值内；==（这个存疑，因为这时当前帧的位姿还没有计算出来，所以转换到世界坐标系下是使用的参考帧位姿，这很不准的）==。

​		**其次：**对于每一对通过筛选的内点，创建一个Correspondence类的对象，注册到*self*.bundler. _fm. _matches中，这个对象的成员变量内就储存着这个匹配点对的信息，包括 `_uA,` `_uB` `_vA`, `_vB`, `_ptA_cam`, `_ptB_cam` `_isinlier`, `_confidence` 。因此， _matches的长度就代表着这两帧有多少个内点匹配对。

- `_uA` :  特征点在帧A图像中的X坐标；
- `_uB` :  特征点在帧B图像中的X坐标；
- `_vA`:   特征点在帧A图像中的Y坐标；
- `_vB`:   特征点在帧B图像中的Y坐标；
- `_ptA_cam`: 在帧A坐标系下对应的点云；  
-  `_ptB_cam`: 在帧B坐标系下对应的点云；
-  `_isinlier`: 是否为内点的bool标志；
-  `_confidence`: 统一为1 ，因为已经是内点。

#### 2.8.6 RANSAC求解位姿前后的特征匹配可视化（检查日志级别SPDLOG > 2）

​		如果检查日志级别> 2 ，那就把特征匹配可视化出来。通过调用self.bundler._fm.rawMatchesToCorres()函数。使用RANSAC求解位姿时会区分内外点，这个函数主要的目的是筛选之后的特征匹配情况。

​		但是在该函数中，==有一些计算资源的浪费==，因为只有当SPDLOG > 2才会可视化，可是当SPDLOG=2时，会计算，但是最后不会绘制出来。

#### 2.8.7  RANSAC + ICP找到最优位姿——runRansacMultiPairGPU()

​		这一步作者的代码写了好几个嵌套函数，这部分的工作用**一句话来概括就是：使用帧对的匹配结果，通过RANSAC+ICP求解最优位姿，使用最优位姿的内点，更新匹配点集。**接下来，我们逐层来讲一讲，这个部分都做了什么。

​                                                                                         **（第一层）**		

​		把当前帧对传入==bundler._fm.runRansacMultiPairGPU()==函数（只有一对帧传入，但是作者的代码写的都是面向多个帧对的向量操作，感觉有点没必要），这个函数包括三个关键步骤：

- 第一步：读取当前帧对的匹配数据 和 RANSACN参数，把他们拷贝在GPU内存的变量中，为GPU上的RANSAC计算做准备。

- 第二步（第二层）：把GPU中的变量传入==ransacMultiPairGPU()==函数，这个函数使用了双重的GPU并行结构，利用RANSAC+ICP筛选内外点，找到最好的那次迭代及其模型。==没有把RANSAC+ICP求解出来的Pose传出来==

- 第三步：根据 RANSAC 筛选的内点结果，更新匹配点集_matches，如果筛选内点之后的匹配点过少，少于了阈值min_match_after_ransac，那么会把 _matches设为空，在后面的流程遇到空的情况，会把这帧注销。

    ------

    ​                                                                               	**（第二层）**

​		ransacMultiPairGPU()这个函数的工作主要是搭建了一个gpu双并行计算的结构，然后先后使用4个核函数，依次并行计算每次ransac迭代、筛选内外点和找到最优迭代。

- **首先，创建CUDA流：**

    ​		对每一个帧对，都创建了一个的CUDA流，用来并行。（虽然代码中m=1）。这样的目的是可以让这些流并行的计算而无需等待前一个操作完成。

- **然后，核函数 1 — ransacEstimateModelKernel() ：RANSAC+SVD求解位姿** 2000轮  

    ​		对于2000轮的RANSAC计算，定义线程块个数4和线程数量512，将GPU核函数ransacEstimateModelKernel()放在该帧对的CUDA流中并行运行。该核函数内容是：从匹配的特征点对中，随机选择三对点 ，构建最小二乘，用SVD分解求解 Rt==(注意这里计算的是当前帧和参考帧之间的RT，是从当前帧转换到参考帧，但却没有把在GPU求解出来的Pose传出来)==。如果R符合要求，将该次的pose注册到放入poses_gpu[m]中 

- **其次，核函数 2  — ransacEvalModelKernel() ：筛选每轮迭代的内点，统计内点数**

    ​		对于每次RANSAC计算之后的所有匹配点内外点筛选，定义线程块个数（63 * 30）和线程数量（32*32）将GPU核函数ransacEvalModelKernel()放在该帧对的CUDA流中并行运行。每一个线程块表示用来处理每一次RANSAC迭代的内外点筛选 ，线程块内的每一个线程表示一对匹配点。该核函数的内容是：基于每次RANSAC迭代之后的pose，筛选内点并计数，采用3D点在参考帧坐标系下的距离和法向量阈值作为筛选凭据，更新内点数n_inliers_gpu 和 每个点的内点标志inlier_flags_gpu成员变量。==（这两个成员变量的格式是从帧对开始的，注意代码里的索引）==

- **再其次：核函数 3 4  —寻找所有迭代中内点数量的最大值，和对应的迭代id**

    ​		通过核函数3 findBestInlier 和核函数4 getBestTrial ，找到了所有迭代中的最大内点数及其对应的那次迭代id，这就相当于找到了最优的模型。

- **最后：将最佳模型的ID号best_trial_id，内点标志inlier_flags和内点索引inlier_ids放在CPU内存上用于之后的匹配点集更新，并释放GPU上的内存。**

    ​		

    ​		第二层的整体结构如下图，核函数没有写完，但是表示的结构是这样。

​	<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/IMG_20231026_215231.jpg" alt="IMG_20231026_215231" style="zoom: 16%;" />



### 2.9 如果这帧被设为了FAIL，那么忘记该帧

​		这是对应着，在2.8.4中初次匹配后匹配点太少被设为了FAIL的情况。



### 2.10 重定位跟踪

​		如果跟踪数量不够，从共视关键帧中从大到小轮流find_corres，直到跟踪数量符合要求，如果还是跟踪不上就Frame判定为FAIL。



### 2.11 获得当前帧的位姿 pose

​		使用筛选后的内点，通过SVD线性求解ICP得到当前帧的位姿。

==（为什么不在当时RANSAC+ICP的时候直接 从gpu中拷贝过来 而是要重新计算？ 打印一下两次计算有没有误差）==

### 2.12 剔除冗余帧Frame

​		如果当前程序的frame数量比keyframe多5 ，那么对bundler._frames进行剔除冗余帧。剔除的规则是，删除bundler. _frames字典在中的非关键帧，那不全剩下关键帧？

​		==为什么要这么做？那self.bundler. _frames 和 self.bundler. _keyframes的区别是啥？ 为了让跟踪参考帧更准一些吗？那为什么不直接跟踪关键帧？==

### 2.13 如果当前帧跟踪成功,注册当前帧在bundler._frames中

​		只有帧跟踪成功了，才注册到记录帧的日志中。 

### 2.14   局部 BA 环节（） 

由于这一部分的代码过于复杂，所以把2.14部分作为一块内容来讲，  ==理论上在这之前要新建地图点，然后很方便的找索引关系，但是这里却每两个局部帧都要两两匹配，这很慢的，但是我也不知道作者是怎么优化的，可能不使用G2o框架就不用找？等回头把那个CUDASolverBundling看完再说==

#### 第一步：找局部关键帧，给局部BA做准备——*self*.bundler.selectKeyFramesForBA()

​		根据不同的筛选策略选择局部关键帧，根据作者的配置采用共视程度来寻找，局部帧一共找10个，这10个中包含一个是当前帧，其他的关键帧按照共视权重的排名，直到塞满10个。

#### 第二步：在局部关键帧中找需要做特征匹配的帧对——bundler.getFeatureMatchPairs

​		对于局部关键帧中的那些 没有匹配过 且 彼此共视程度满足要求的帧 放入pairs中，需要进行补充特征匹配，用于接下来的局部BA优化。

​		==不太理解，是因为没有新建地图点吗还是？为什么要每两个帧都做特征匹配用来找索引关系，可能作者使用的CUDA优化里面需要吧，先跳过==

#### 第三步：做特征匹配——find_corres

​		把需要做特征匹配的帧对放入find_corres中，寻找两两特征匹配并使用RANSAC+ICP筛选内点 

#### 第四步：在GPU上局部BA优化——Bundler::optimizeGPU

​		到了这一步，我们已经得到待优化的局部帧位姿和每一帧的匹配关系，理论上我们只需要基于匹配关系找到每一个地图点的共视关系，就可以通过g2o框架来优化了。但是作者这里写的十分复杂，函数的嵌套很多，我们先按照作者的思路来梳理优化过程。具体过程请见单独写的记录文件Bundler::optimizeGPU.md文件。只需要知道在优化就行。

### 2.15 检查是否添加关键帧  

如果当前帧成功先后经过参考帧跟踪和局部BA，那么检查是否添加当前帧为关键帧  

 

| 变量             | 类型                          | 意义                                                         |
| ---------------- | ----------------------------- | ------------------------------------------------------------ |
| depths_gpu       | std::vector<float*>           | 储存所有局部帧深度图的向量                                   |
| colors_gpu       | std::vector<uchar4*>          | 储存所有局部帧彩色图的向量                                   |
| normals_gpu      | std::vector<float4*>          | 储存所有局部帧点云法线向量图的向量                           |
| poses            | std::vector< Eigen::Matrix4f> | 储存所有局部帧poses的向量                                    |
| n_match_per_pair | std::vector< int>             | 存储每对帧之间的特征匹配数量的向量                           |
| global_corres    | std::vector< EntryJ>          | 保存每对帧之间的匹配信息<br />(两个帧的索引、匹配点的法线向量、3D位置信息) |
|                  |                               |                                                              |

## step 3   如果当前帧被设为关键帧，准备数据给nerf进程派活

​		如果当前帧被设为关键帧了，那么就把当前帧的数据，放入p_dict 和 kf_to_nerf_list中用于与nerf进程通信。其中比较关键的是把自身的color，depth和mask数据构造成一个字典，添加到kf_to_nerf_list中。kf_to_nerf_list的长度决定是否启动nerf环节，当长度到达我们给定的阈值后，nerf就会被启动。

​		nerf启动时，会把p_dicrt中的running值设为True，表示启动了，然后开始执行nerf进程的主要工作。在nerf工作中会把kf_to_nerf_list的数据存入nerf进程的变量中，然后清空当前kf_to_nerf_list的数据，处理这一部分输入的数据。在处理完之后会再次检查kf_to_nerf_list的长度，如果长度>0，就会处理这些被传入的数据，而无需让kf_to_nerf_list的长度达到阈值，nerf如此反复，直至处理完所有帧或停止。

## step 4  更新nerf优化后的关键帧位姿，剔除优化前后位姿变化大的帧的匹配信息

​		把经过nerf优化之后的位姿更新到bundler. _keyframes. _pose_in_model中，并将对应帧标记为nerfed，意为已经过nerf优化了在之后的局部BA中不优化。然后计算nerf优化前后的位姿变化，对于变化比较大的，就剔除他的特征匹配数据（从 _match中剔除包含该帧的帧对匹配数据）==不太懂为什么要剔除==



## step 5 保存该帧跟踪结果

​		用于在处理新帧数据后保存相关信息，包括位姿、图像、深度数据和点云等。具体的操作和文件路径根据配置和日志级别来决定是否执行。主要内容有以下方面：

1. 打印日志信息，记录 "Welcome saveNewframeResult"。
2. 获取一些配置参数和目录路径，包括 `K_file`、`debug_dir`、`out_dir` 和 `pose_out_dir`。
3. 检查是否已存在 `K_file` 文件，如果不存在，则创建并写入 `_newframe->_K` 的数据。
4. 检查是否已存在 `pose_out_dir` 目录，如果不存在，则使用系统命令 `mkdir` 创建一系列子目录，如 "color"、"color_viz"、"color_keyframes" 等。
5. 计算当前帧在模型坐标系中的位姿 `ob_in_cam`，并将其保存到文件中。
6. 如果配置中指定了足够详细的日志记录（`SPDLOG` 大于等于1），则执行以下操作：
    - 将 `_newframe->_id_str` 的颜色图像保存为 PNG 文件。
    - 将深度图像转换为 16 位无符号整数并保存为 PNG 文件。
    - 将深度滤波后的图像保存为 PNG 文件。
    - 创建一个掩码图像，将前景像素设置为255，并保存为 PNG 文件。
    - 创建一个法线图像，将法线向量映射到颜色值，并保存为 PNG 文件。
    - 创建一个颜色分割的图像，保存到名为 "color_segmented" 的子目录中。
7. 如果 `SPDLOG` 仍然大于等于1，将一些额外的图像和文件保存到目录中，包括 "depth_vis" 图像、"opt_frames.txt" 文件、关键帧的位姿信息以及帧的状态和参考帧信息。
8. 如果 `SPDLOG` 大于等于4，执行更多操作：
    - 创建点云数据 `cloud_raw_depth`，并将它保存为 PLY 文件。
    - 创建点云数据 `cloud_world_gt`，并将它保存为 PLY 文件。
9. 打印日志信息，记录 "saveNewframeResult done"，表示该函数的执行已完成。







# GUI进程

​		在BundleSDF初始化的时候创建了run_gui进程，这个进程主要用于以窗口的形式可视化每一帧的位姿估计及重建。run_gui进程主要包含两个部分，第一个是GUI初始化，该部分是对于GUI窗口中的大小，布局，文字和渲染器等的静态设置；第二个是动态更新，这部分是基于每一帧处理并传输至gui_dict中的共享数据，动态的更新每一帧的位姿表示。

​		GUI初始化：使用BundleSdfGui类完成，主要工作是对 GUI 界面和界面相关的属性设计。

​		动态更新：基于gui_dict数据，跟随该共享数据的更新使用update_frame方法更新GUI 界面中的帧图像、遮罩、物体位姿等信息的显示。

​		在动态更新的过程中，比较重要的是update_frame方法，这个方法中更新每一帧的位姿轴线，和图像纹理，设定了用于可视化的初始XYZ轴线是[1,0,0]  [0,1,0]  [0,0,1]  和原点[0,0,0] 。因此这对于我们的跟踪算法提出要求，即：不能把第一帧的点云当作世界坐标系的点云，对于第一帧的点云也要计算他和我们定义的初始世界坐标系之间的位姿变化。在算法中，作者把点云的质心作为了初始位姿中心，把？？当作初始旋转中心（好像没看到）。







# NeRF进程

## 做了什么

​		nerf环节实际上是使用了Instant-NGP 用来在线学习对象的神经隐式表示 ，为什么可以在线学习，因为Instant-NGP对NeRF的传统学习流程做了改进，通过对位置信息xyz添加多分辨率的哈希编码，用稀疏的参数化的voxel grid作为场景表达，能够加速pipeline使其达到将近实时的水平。

​		nerf环节使用的帧是经过局部BA优化后的关键帧，在关键帧的数量达到某个阈值时，我们启动NeRF流程，分批次把关键帧输入进来学习和优化。Instant-NGP输入的数据是每个关键帧中每个点云点的位置信息（xyz） 及其对应的视角信息（阿尔法 seita） ，输出是该点的颜色rgb和密度。Instant-NGP包括密度网络和外观网络，其中密度网络仅仅使用位置信息训练，外观网络使用密度网络的隐向量cat视角信息特征学习训练。

## 何时启动

​		在kf_to_nerf_list的长度达到设定阈值时启动NeRF流程，kf_to_nerf_list中储存着每一个用于NeRF训练的关键帧的数据，包括color depth和  mask 。除此之外tracking流程还会把所有的关键帧位姿cam_in_obs传入NeRF流程。NeRF流程每次处理kf_to_nerf_list序列中的关键帧，处理完之后清空，如此反复。但是cam_in_obs是不清空的，因此在NeRF流程中对于pose的索引是比较讲究的。

![image-20231108105924274](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231108105924274.png)

## Step 1.a 如果是第一次启动NeRF

### 1.1 生成、处理、合并、下采样点云，将合并后的点云移动到原点和归一化 compute_scene_bounds

​		通过Parallel并行对kf_to_nerf_list的每一帧基于Mask生成和处理点云，然后把他们位姿变换到世界坐标系下合并在一起，合并之后首先进行体素下采样，然后进行二聚类，通过聚类使点云去噪。对于去噪的点云再次根据质心计算原点位移(在Tracking环节已经转换到世界坐标系了，因此这个位移应很小) 和点云坐标xyz归一化到[-1 , 1]区间的缩放系数。然后通过位移和缩放系数构造相似变换矩阵，将点云xyz值归一化到[-1,1]之间,点云中心移动至[000]。

1.1.1 (GPU并行)每一帧基于mask rgb depth生成和处理点云 (compute_scene_bounds_worker)  

​		==这里个人认为可以直接把点云从Track流程通过*kf_to_nerf_list*传进来，这样就避免了让每一帧再重新生成点云，只需要后处理即可==

1.1.2 把所有帧的点云对象转换到世界坐标系下合并在一起进行体素下采样

1.1.3 对合并的点云聚类过滤噪声后 计算到原点位移(在Tracking环节已经转换到世界坐标系了，因此很小)和归一化系数，通过相似变换矩阵，将点云xyz值归一化到[-1,1]之间,点云中心移动至[ 000 ] 

聚类和点云去噪的先后对比如下：好像并没有什么区别？（30个关键帧之后的初始Nerf环节点云）

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113141344443.png" alt="image-20231113141344443" style="zoom: 50%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113141453624.png" alt="image-20231113141453624" style="zoom: 50%;" />

1.1.4 返回值 ， 返回值有四个，下面会将这些返回值注册到NeRF的配置和成员变量中。

### 1.2 注册用于点云归一化的相关变量

​		基于1.1流程的返回值，注册归一化系数 和 到原点的平移到cfg配置 注册相似变换矩阵到成员变量tf_normalize

    *  sc_factor   : 归一化的缩放系数 ，注册到cfg_nerf['sc_factor']中。
    *  translation : 世界坐标系下已经合并的点云距原点的位移，注册到cfg_nerf['translation']中。
    *  (pcd_all) pcd_real_scale : 世界坐标系下合并,但没有归一化和平移的真实尺度点云数据副本，注册到pcd_all中 
    *  pcd_normalized : 世界坐标系下合并后，归一化到[-1,1]之间，中心移动至[000]的点云
    *  tf_normalize   : 世界坐标系下对合并点云做归一化和移动的相似变换矩阵（这个是在1.2中重新计算的，感觉重复了，把1.1中计算的返回就可以了）



## Step 1.b 如果不是第一次启动NeRF  







## Step 2  归一化每一帧的color depth和pose 为NeRF训练做准备

​		归一化是为了NeRF训练做准备，因为训练过程中使用的输入数据是归一化之后的有利于训练过程的稳定性。

​		基于点云的缩放系数sc_factor和平移距离translation归一化每一帧的位姿： 之前每一帧的位姿在Tracking中跟踪，世界坐标系的原点是初始帧点云的质心平移距离，还并不是合并之后的点云质心，因此会存在偏差。在合并之后重新计算了质心平移，将每一帧的位姿更新以瞄准合并点云的质心位置，同时对每一帧姿态的旋转矩阵乘以缩放系数进行归一化。其中位姿数据在训练完成和优化之后，应该会保留它的位移而消去他的缩放系数。这样就得到了每一帧NeRF优化之后指向合并点云对象质心的位姿。

​		归一化color和depth：将颜色像素值和深度范围归一化到[-1 , 1] 

```
rgbs = (rgbs / 255.0).astype(np.float32)
depths *= sc_factor
```

​		

## Step 3.a 创建NerfRunner 对象(第一次启动NeRF)

​		如果是第一次启动NeRF，那么创建NeRF对象，并把当前kf列表中的关键帧数据添加进去准备训练。

​		如果不是第一次启动，那么就添加当前kf列表中的关键帧数据准备训练。

### 3.1 如果下采样倍率不等于 1，则将图像、深度、mask和K等下采样和调整

### 3.2 把输入的点云坐标膨胀后建立八叉树—build_octree

​		输入的点云是归一化和移动到原点之后的pcd_normalized，使用点云的(pcd_normalized.points)坐标点数据创建八叉树，pcd_normalized.points在转化到八叉树表示之前，先进行膨胀处理，膨胀处理之后初始化八叉树管理器。在初始化八叉树管理器的时候根据点云的坐标创建了八叉树，这里是使用Kaolin库将点云坐标转化为八叉树表示。 ==详见build_octree()==

膨胀前后的点云对比：

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113143330636.png" alt="image-20231113143330636" style="zoom:33%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113143530795.png" alt="image-20231113143530795" style="zoom: 33%;" />

八叉树的体素框  （左octree_boxes_max_level.ply）（右 octree_boxes_ray_tracing_level）

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113144623485.png" alt="image-20231113144623485" style="zoom:33%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113144440229.png" alt="image-20231113144440229" style="zoom:40%;" />

### 3.3 创建神经辐射场模型——create_nerf

3.3.1 构建编码器用于给输入的数据编码

（对坐标使用了多分辨率哈希，对方向使用？球形编码？）

3.3.2 初始化粗和细NeRF网络

​		==并没有使用精细网络，只使用了一个粗网络，也就是说没有进行分层采样。原因还不知道，可能instant中没有使用，还是因为影响速度？==

3.3.3 创建用于优化位姿的PyTorch.Parameter

​		基于当前输入帧的数目注册一个pytorch参数，该参数是大小为 [num_frames, 6] 的张量，初始值为零。后面用来存储每帧的姿态参数，在Nerf训练中同步反向传播和优化位姿。

### 3.4 创建训练NeRF的优化器Adam——create_optimizer()

​		使用Adam作为优化器，把待优化参数添加到优化器中。



### 3.5 生成每一帧光线——make_frame_rays

​		**一句话总结：首先生成每一帧在Ｍask和Ｄepth有效区域的像素光线（当前帧坐标系下），其次给每一条光线计算取样区间（通过该光线在世界坐标系下与点云边界框的远近交点），最后通过判断该光线在世界坐标系下与八叉树结构是否有交点，进行筛选。最终得到过滤后的（Ｎ，n）格式的光线。**

![image-20231114093148586](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231114093148586.png)

3.5.1生成该帧每个像素的光线 

​		生成每一帧中每一个像素对应的光线dir，初此之外，还把rgb、mask、depth等数据也添加进去，这里的光线是[H ,W ,n]格式的。

![image-20231113212247183](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113212247183.png)

3.5.2 膨胀掩码Mask，以填充掩码区域

​		这里对传入的Mask掩码进行膨胀处理，作者可能是担心掩码在边缘有一些缺失，从而遗漏了一些光线的生成。

==看看膨胀前后的变换==

3.5.3 基于像素点的深度值和Mask，筛选光线并展平

​		对于Mask值==0 或 depth值>2  depth值<1的像素点，剔除3.5.1中[H ,W ,n]格式的光线。剔除后将其展平为形状为 (N, n) 的数组。由于Mask比较小，N应该远小于H\*W。

==看看是否存在depth不合法的值，理论上经过之前的处理都已经合法==

3.5.4 计算每条光线的取样区间

​		计算世界坐标系下每条光线与点云边界框[[-1,-1,-1], [1,1,1]] 的交点，将近点和远点作为取样区间，并把近点和远点也加进光线数据中，之后采样在每条光线的近点和远点内采，避免采样到点云之外的点。

 3.5.5 利用八叉树的结构，筛选有效的光线

​		判断光线在场景中是否和物体有交点，如果没有交点，那么这个光线是无效的，剔除掉这些无效的光线。

### 3.6 根据八叉树云数据对点云进行深度去噪，识别和修复一些被认为是无效深度的点

### 3.7 使用构造和筛选好的光束rays创建 DataLoader对象，用于训练时采样。

​		在DataLoader函数中，会随机打乱数据集，输入数据时只需要调用DataLoader.ids取样即可，批次大小N_rand为2048。

## Step 3.b 更新 NerfRunner 对象(不是第一次启动NeRF)







## Step 4 Nerf 模型训练

​		沿光线取样，进入NeRF模型训练

## NeRF进程成员变量

| 变量            | 类型         | 意义                                                         |
| --------------- | ------------ | ------------------------------------------------------------ |
| nerf_num_frames | int          | 统计驱动nerf的关键帧总数目，传给共享内存p_dict               |
| cnt_nerf        | int          | 用于记录NeRF启动的次数，初始值为-1，当关键帧攒够10个后启动，为0 |
| cam_in_obs      | list< pose > | 储存全部关键帧的位姿（Tracking流程非线性优化出的）           |
| glcam_in_obs    | list< pose > | 用于NeRF训练的位姿，cam_in_obs的位姿先取反，旋转矩阵 *归一化系数factor ，平移t部分加上合并点云中心到世界坐标系原点[0,0,0]的距离 glcam_in_obs = cam_in_obs@glcam_in_cvcam |
| sc_factor       |              |                                                              |
| translation     |              |                                                              |
| tf_normalize    |              | 世界坐标系下对合并点云做归一化和移动的相似变换矩阵           |
| pcd_all         |              | 所有帧合并后，但没有归一化和平移的真实尺度点云数据副本       |
| pcd_normalized  |              | 所有帧合并后的，归一化到[-1,1]之间，中心移动至[000]的点云    |
| occ_masks       |              | 是什么？                                                     |
|                 |              |                                                              |
|                 |              |                                                              |
|                 |              |                                                              |



# 不太明白的地方



## 第一帧的位姿定义？

​		第一帧的旋转R定义为单位旋转，所以问题是，我们无法保证第一帧的旋转就是我们期望定义的初始旋转，可能要等地图点重建好之后，在手动设计初始旋转。

​		第一帧的位置T是按照点云的质心位置计算的，这个理论上没什么问题。

## 把每个地方的用时打印出来看看

Output每一帧文件夹下面的文件都是在哪里保存的，是什么信息？

细致一些，除了2.14还有哪些地方最费时间

step 5 保存文件的环节特别费时间，把其他地方保存文件的地方也注释掉看一看





# 接下来的工作：

2.22的工作就是对于匹配点不够的情况，调整了一下ransacEvalModelKernel函数的内外点筛选，发现本来匹配了很多，但是筛选之后就变得特别少，本来把余弦相似度删掉，但是发现不准了，于是加上发现巨准，但是匹配不够，现在准备调参，调条件一和条件二的阈值直到能用。

hubble 0002  0003效果都不错 

1. 多个航天器，定量和定性的图生成一下  （明天做这个） 做图和表
2. 重建的质量不好，只有一个主体，没有翅膀（认为是位姿不准导致的，所以在关键帧之前修改平移，让关键帧的位姿比较准确）但是报错了 回来改一下艾 
3. 匹配过程中的报错问题处理一下（很有可能是关键帧比较少）
4. ransac的内点阈值有点高，可以选择放宽  ( 放宽了变换点的距离和余弦相似度) ， 并且在RANSAC之前的匹配筛选也放宽了。 放宽会导致不准，这个参数需要手调
5. 对称问题，关键帧的范围宽容一些，然后优化的帧选取最近的10个而不是匹配最多的10个 ，可能会好一些

## 1.看nerf做了什么,理解和学习nerf，instantgnp代码   

传入NeRF流程的数据，都搞清楚，有些变换来变换去的。

点云的坐标、深度图的坐标，mask的前后变化，每个光线在点云或八叉树结构中的可视化



## 3.优化代码用于加速，并尝试理解姿态图优化代码

改一下shorter_side ，使其图像大小变为300 ，更小一些

step 1  构造帧 这么费时 其中点云的包围盒没必要做



2.2  基于掩码的更新如何加速？  费时主要是遍历了所有图像的像素，该怎么加速呢？



2.8  尝试用CUDA加速Loftr环节。或把Loftr中的特征点稀疏一些。每次900多个太多了。



2.9-2.11 直接把2.8中的位姿传回来。

**在ransacMultiPairGPU核函数中，找到最好的pose模型并注册和返回（先打印出来看看，做个对比）,如果一样，把后面的删掉**



2.14 使用一个地图点对象避免每一帧都要做特征匹配。（要熟悉CUDA nerf优化的方法才行）

2.14 每帧都要做特征匹配就算了，竟然还要每帧都RANSAC+ICP求每两帧的相对位姿，确实没有看明白，把优化过程看明白了之后，在findcorres中加个判断，对于只有一对帧来说，需要匹配+RANSACICP计算位姿，对于多对帧，因为已经有位姿了，仅仅是匹配用来优化，不进行RANSAC+ICP.

step 5 很费时间，看看里面是不是保存文件太费时间了，把其他的地方删掉。

取消 step 5 每帧加速500ms

取消GUI，每帧加速30ms

## 5.写论文







## 什么时候使用CUDA加速了？

1. 在多个帧对使用RANSAC+ICP跟踪单帧位姿和筛选匹配的内点时使用了，这个是使用CUDA流并行的，每一个帧对开一个流，然后在每一个流中使用多个线程并行。

2. 在局部BA 下采样的时候使用CUDA_cache调用CUDA加速了。
3. 优化的时候使用CUDASloverBundling加速了，这个我还没有看明白是什么加速的。

## 插入关键帧的时机

第一帧作为关键帧，后面每一个跟踪成功和SBA优化之后的帧都作为关键帧

## 不解的地方

我都在代码中标注了  //？

最大的问题是，初始位姿的设置，在没有点云先验的情况下，如何设置参考位姿？ 在SLAM中，跟踪局部地图的效果很好，但这里没有使用跟踪局部地图，为什么？



## 最耗时的地方

2.14中选择用于局部BA的关键帧、getFeatureMatchPairs和局部帧find_corres三个步骤，非常耗时。==认为是计算共视分数和特征匹配时间太长==

有几个思路：

1.把Loftr换成更快的特征匹配
2.看懂优化过程中需要的变量，然后避免使用每两个帧匹配的方法，通过引入地图点的对象，只做一个匹配，就能够获得对应关系。必须要引入地图点对象，不然太慢了

3.把他的后端放orbslam2中

4.把选择用于局部BA的关键帧、getFeatureMatchPairs两个步骤，直接使用最近的10个关键帧代替，不进行共视图权重的筛选，考虑到卫星都是匀速转动的。





## 要学会的地方

CUDA加速

Instant NGP的加速思想

一个以对象为中心的NeRF-SLAM框架
