[TOC]



# NeRF进程

## 做了什么

​		nerf环节实际上是使用了Instant-NGP 用来在线学习对象的神经隐式表示 ，为什么可以在线学习，因为Instant-NGP对NeRF的传统学习流程做了改进，通过对位置信息xyz添加多分辨率的哈希编码，用稀疏的参数化的voxel grid作为场景表达，能够加速pipeline使其达到将近实时的水平。

​		nerf环节使用的帧是经过局部BA优化后的关键帧，在关键帧的数量达到某个阈值时，我们启动NeRF流程，分批次把关键帧输入进来学习和优化。Instant-NGP输入的数据是每个关键帧中每个点云点的位置信息（xyz） 及其对应的视角信息（阿尔法 seita） ，输出是该点的颜色rgb和密度。Instant-NGP包括密度网络和外观网络，其中密度网络仅仅使用位置信息训练，外观网络使用密度网络的隐向量cat视角信息特征学习训练。

## 何时启动

​		在kf_to_nerf_list的长度达到设定阈值时启动NeRF流程，kf_to_nerf_list中储存着每一个用于NeRF训练的关键帧的数据，包括color depth和  mask 。除此之外tracking流程还会把所有的关键帧位姿cam_in_obs传入NeRF流程。NeRF流程每次处理kf_to_nerf_list序列中的关键帧，处理完之后清空，如此反复。但是cam_in_obs是不清空的，因此在NeRF流程中对于pose的索引是比较讲究的。

![image-20231108105924274](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231108105924274.png)

## Step 1.a 如果是第一次启动NeRF

### 1.1 生成、处理、合并、下采样点云，将合并后的点云移动到原点和归一化 compute_scene_bounds

​		通过Parallel并行对kf_to_nerf_list的每一帧基于Mask生成和处理点云，然后把他们位姿变换到世界坐标系下合并在一起，合并之后首先进行体素下采样，然后进行二聚类，通过聚类使点云去噪。对于去噪的点云再次根据质心计算原点位移(在Tracking环节已经转换到世界坐标系了，因此这个位移应很小) 和点云坐标xyz归一化到[-1 , 1]区间的缩放系数。然后通过位移和缩放系数构造相似变换矩阵，将点云xyz值归一化到[-1,1]之间,点云中心移动至[000]。

1.1.1 (GPU并行)每一帧基于mask rgb depth生成和处理点云 (compute_scene_bounds_worker)  

​		==这里个人认为可以直接把点云从Track流程通过*kf_to_nerf_list*传进来，这样就避免了让每一帧再重新生成点云，只需要后处理即可==

1.1.2 把所有帧的点云对象转换到世界坐标系下合并在一起进行体素下采样

1.1.3 对合并的点云聚类过滤噪声后 计算到原点位移(在Tracking环节已经转换到世界坐标系了，因此很小)和归一化系数，通过相似变换矩阵，将点云xyz值归一化到[-1,1]之间,点云中心移动至[ 000 ] 

聚类和点云去噪的先后对比如下：好像并没有什么区别？（30个关键帧之后的初始Nerf环节点云）

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113141344443.png" alt="image-20231113141344443" style="zoom: 50%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113141453624.png" alt="image-20231113141453624" style="zoom: 50%;" />

1.1.4 返回值 ， 返回值有四个，下面会将这些返回值注册到NeRF的配置和成员变量中。

### 1.2 注册用于点云归一化的相关变量

​		基于1.1流程的返回值，注册归一化系数 和 到原点的平移到cfg配置 注册相似变换矩阵到成员变量tf_normalize

    *  sc_factor   : 归一化的缩放系数 ，注册到cfg_nerf['sc_factor']中。
    *  translation : 世界坐标系下已经合并的点云距原点的位移，注册到cfg_nerf['translation']中。
    *  (pcd_all) pcd_real_scale : 世界坐标系下合并,但没有归一化和平移的真实尺度点云数据副本，注册到pcd_all中 
    *  pcd_normalized : 世界坐标系下合并后，归一化到[-1,1]之间，中心移动至[000]的点云
    *  tf_normalize   : 世界坐标系下对合并点云做归一化和移动的相似变换矩阵（这个是在1.2中重新计算的，感觉重复了，把1.1中计算的返回就可以了）







## Step 1.b 如果不是第一次启动NeRF  





















## Step 2  归一化每一帧的color depth和pose 为NeRF训练做准备

​		归一化是为了NeRF训练做准备，因为训练过程中使用的输入数据是归一化之后的有利于训练过程的稳定性。

​		基于点云的缩放系数sc_factor和平移距离translation归一化每一帧的位姿： 之前每一帧的位姿在Tracking中跟踪，世界坐标系的原点是初始帧点云的质心平移距离，还并不是合并之后的点云质心，因此会存在偏差。在合并之后重新计算了质心平移，将每一帧的位姿更新以瞄准合并点云的质心位置，同时对每一帧姿态的旋转矩阵乘以缩放系数进行归一化。其中位姿数据在训练完成和优化之后，应该会保留它的位移而消去他的缩放系数。这样就得到了每一帧NeRF优化之后指向合并点云对象质心的位姿。

​		归一化color和depth：将颜色像素值和深度范围归一化到[-1 , 1] 

```
rgbs = (rgbs / 255.0).astype(np.float32)
depths *= sc_factor
```

​		











## Step 3.a 创建NerfRunner 对象(第一次启动NeRF)

​		如果是第一次启动NeRF，那么创建NeRF对象，并把当前kf列表中的关键帧数据添加进去准备训练。

​		如果不是第一次启动，那么就添加当前kf列表中的关键帧数据准备训练。

### 3.1 如果下采样倍率不等于 1，则将图像、深度、mask和K等下采样和调整

### 3.2 把输入的点云坐标膨胀后建立八叉树—build_octree

​		输入的点云是归一化和移动到原点之后的pcd_normalized，使用点云的(pcd_normalized.points)坐标点数据创建八叉树，pcd_normalized.points在转化到八叉树表示之前，先进行膨胀处理，膨胀处理之后初始化八叉树管理器。在初始化八叉树管理器的时候根据点云的坐标创建了八叉树，这里是使用Kaolin库将点云坐标转化为八叉树表示。 ==详见build_octree()==

膨胀前后的点云对比：

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113143330636.png" alt="image-20231113143330636" style="zoom:33%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113143530795.png" alt="image-20231113143530795" style="zoom: 33%;" />

八叉树的体素框  （左octree_boxes_max_level.ply）（右 octree_boxes_ray_tracing_level）

<img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113144623485.png" alt="image-20231113144623485" style="zoom:33%;" /><img src="/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113144440229.png" alt="image-20231113144440229" style="zoom:40%;" />

### 3.3 创建神经辐射场模型——create_nerf

3.3.1 构建编码器用于给输入的数据编码

（对坐标使用了多分辨率哈希，对方向使用？球形编码？）

3.3.2 初始化粗和细NeRF网络

​		==并没有使用精细网络，只使用了一个粗网络，也就是说没有进行分层采样。原因还不知道，可能instant中没有使用，还是因为影响速度？==

3.3.3 创建用于优化位姿的PyTorch.Parameter

​		基于当前输入帧的数目注册一个pytorch参数，该参数是大小为 [num_frames, 6] 的张量，初始值为零。后面用来存储每帧的姿态参数，在Nerf训练中同步反向传播和优化位姿。

   ==为每一帧设置一个可学习的六自由度pose参数，这个参数初始值为0，为什么不把跟踪的位姿输入进来，而是要0初始值？==

答复：是学习优化的位姿残差值，残差值是用李群表示的，所以是0。会通过get_matrices方法转化到矩阵的形式。

### 3.4 创建训练NeRF的优化器Adam——create_optimizer()

​		使用Adam作为优化器，将3.3创建NeRF中的编码器,网络结构和pose_array参数都放入优化器待优化的参数中

![image-20231117134921529](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231117134921529.png)

### 3.5 生成每一帧光线——make_frame_rays

​		**一句话总结：首先生成每一帧在Ｍask和Ｄepth有效区域的像素光线（当前帧坐标系下），其次给每一条光线计算取样区间（通过该光线在世界坐标系下与点云边界框的远近交点），最后通过判断该光线在世界坐标系下与八叉树结构是否有交点，进行筛选。最终得到过滤后的（Ｎ，n）格式的光线。**

![image-20231114093148586](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231114093148586.png)

3.5.1生成该帧每个像素的光线 

​		生成每一帧中每一个像素对应的光线dir，初此之外，还把rgb、mask、depth等数据也添加进去，这里的光线是[H ,W ,n]格式的。

![image-20231113212247183](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231113212247183.png)

3.5.2 膨胀掩码Mask，以填充掩码区域

​		这里对传入的Mask掩码进行膨胀处理，作者可能是担心掩码在边缘有一些缺失，从而遗漏了一些光线的生成。

==看看膨胀前后的变换==

3.5.3 基于像素点的深度值和Mask，筛选光线并展平

​		对于Mask值==0 或 depth值>2  depth值<1的像素点，剔除3.5.1中[H ,W ,n]格式的光线。剔除后将其展平为形状为 (N, n) 的数组。由于Mask比较小，N应该远小于H\*W。

==看看是否存在depth不合法的值，理论上经过之前的处理都已经合法==

3.5.4 计算每条光线的取样区间，并把近点和远点也加进光线数据中

​		计算世界坐标系下每条光线与点云边界框[[-1,-1,-1], [1,1,1]] 的交点，将近点和远点作为取样区间，并把近点和远点也加进光线数据中，之后采样在每条光线的近点和远点内采，避免采样到点云之外的点。

 3.5.5 利用八叉树的结构，筛选有效的光线

​		判断光线在场景中是否和物体有交点，如果没有交点，那么这个光线是无效的，剔除掉这些无效的光线。

### 3.6 根据八叉树云数据对点云进行深度去噪，识别和修复一些被认为是无效深度的点

### 3.7 使用构造和筛选好的光束rays创建 DataLoader对象，用于训练时采样。

​		在DataLoader函数中，会随机打乱数据集，输入数据时只需要调用DataLoader.ids取样即可，批次大小N_rand为2048。

## Step 3.b 更新 NerfRunner 对象(不是第一次启动NeRF)



























## Step 4 NeRF 模型训练

### 4.1 开启N_iters轮的训练循环

​		开启训练循环，总轮数为N_iters(500)轮，每一轮随机采样一个批次大小(2048)的光线数据，进入训练。这里有个问题是我们每一帧生成和筛选后的光线数量大概有大几万个，所以对于首次启动NeRF来说，十几个关键帧就可以有100万个光线数据了，如果再多，500轮的训练次数已经不够了。所以首次启动NeRF的关键帧应该不能太多，不然无法充分利用关键帧中的光线数据。

### 4.2 对于这一批次的数据，采样点+前向传播+体渲染

​		将这一批次的数据依次 采样地图点 + MLP前向传播 +体渲染 得到输出。这里面的内容比较多，但是和NeRF大同小异，首先是在之前定义的光线采样近点和远点之间采样，采样后进行MLP网络的前向传播，得到预测值，再将预测值通过体渲染公式的积分求解，得到最终预测的rgb和其他值。不同的是对于得到的其他值，作者使用了其中的sdf值用来计算loss。

### 4.3 计算Loss

​		Loss值有很多，暂时和论文中的还没有对应上，除了辐射值的Loss，其他的都还没有理解，回来的时候要看一看。

==理解各个损失值==



### 4.4 计算梯度和优化（网络参数和位姿）

​		使用梯度缩放器计算并反向传播损失，然后更新模型参数。==梯度缩放器是什么？为什么要使用梯度缩放器==    ==优化变量是什么？ 网络参数和位姿？== ==进去看源码==

![image-20231116215016281](/home/hanbing/公共的/typora实验记录/Bundle SDF/assets/image-20231116215016281.png)

### 4.5 根据NeRF训练的全局次数，选择进行学习率调整、保存权重和保存图像、网格与姿态

​		要注意这里的全局次数不是Nerf的训练轮次，而是指Tracking流程传递给Nerf训练的次数。每10次会进行学习率的调整，对于保存权重等选项，作者把这些都设置为了99999,也就是说除了学习率，其他的都不保存。



## Step 5 得到优化后的位姿

​		优化后的残差位姿从李群转化为矩阵，然后乘以Track的初始位姿，得到优化后的位姿。





## step 6 得到NeRF的mesh

​		NeRF经过学习后，把NeRF网络转换为显示的表示





## NeRF进程成员变量

| 变量            | 类型         | 意义                                                         |
| --------------- | ------------ | ------------------------------------------------------------ |
| nerf_num_frames | int          | 统计驱动nerf的关键帧总数目，传给共享内存p_dict               |
| cnt_nerf        | int          | 用于记录NeRF启动的次数，初始值为-1，当关键帧攒够10个后启动，为0 |
| cam_in_obs      | list< pose > | 储存全部关键帧的位姿（Tracking流程非线性优化出的）           |
| glcam_in_obs    | list< pose > | 用于NeRF训练的位姿，cam_in_obs的位姿先取反，旋转矩阵 *归一化系数factor ，平移t部分加上合并点云中心到世界坐标系原点[0,0,0]的距离 glcam_in_obs = cam_in_obs@glcam_in_cvcam |
| sc_factor       |              |                                                              |
| translation     |              |                                                              |
| tf_normalize    |              | 世界坐标系下对合并点云做归一化和移动的相似变换矩阵           |
| pcd_all         |              | 所有帧合并后，但没有归一化和平移的真实尺度点云数据副本       |
| pcd_normalized  |              | 所有帧合并后的，归一化到[-1,1]之间，中心移动至[000]的点云    |
| occ_masks       |              | 是什么？                                                     |
|                 |              |                                                              |
|                 |              |                                                              |
|                 |              |                                                              |









# 不太明白的地方



## 第一帧的位姿定义？

​		第一帧的旋转R定义为单位旋转，所以问题是，我们无法保证第一帧的旋转就是我们期望定义的初始旋转，可能要等地图点重建好之后，在手动设计初始旋转。

​		第一帧的位置T是按照点云的质心位置计算的，这个理论上没什么问题。

## 把每个地方的用时打印出来看看

Output每一帧文件夹下面的文件都是在哪里保存的，是什么信息？

细致一些，除了2.14还有哪些地方最费时间

step 5 保存文件的环节特别费时间，把其他地方保存文件的地方也注释掉看一看





# 接下来的工作：

## 1.看nerf做了什么,理解和学习nerf，instantgnp代码   

传入NeRF流程的数据，都搞清楚，有些变换来变换去的。

点云的坐标、深度图的坐标，mask的前后变化，每个光线在点云或八叉树结构中的可视化

## 2.在我们自己的数据集上测试

开整自己的仿真数据集！

## 3.优化代码用于加速，并尝试理解姿态图优化代码

改一下shorter_side ，使其图像大小变为300 ，更小一些

step 1  构造帧 这么费时 其中点云的包围盒没必要做



2.2  基于掩码的更新如何加速？  费时主要是遍历了所有图像的像素，该怎么加速呢？



2.8  尝试用CUDA加速Loftr环节。或把Loftr中的特征点稀疏一些。每次900多个太多了。



2.9-2.11 直接把2.8中的位姿传回来。

**在ransacMultiPairGPU核函数中，找到最好的pose模型并注册和返回（先打印出来看看，做个对比）,如果一样，把后面的删掉**



2.14 使用一个地图点对象避免每一帧都要做特征匹配。（要熟悉CUDA nerf优化的方法才行）

2.14 每帧都要做特征匹配就算了，竟然还要每帧都RANSAC+ICP求每两帧的相对位姿，确实没有看明白，把优化过程看明白了之后，在findcorres中加个判断，对于只有一对帧来说，需要匹配+RANSACICP计算位姿，对于多对帧，因为已经有位姿了，仅仅是匹配用来优化，不进行RANSAC+ICP.

step 5 很费时间，看看里面是不是保存文件太费时间了，把其他的地方删掉。



取消 step 5 每帧加速500ms

取消GUI，每帧加速30ms



## 4.写论文







## 什么时候使用CUDA加速了？

1. 在多个帧对使用RANSAC+ICP跟踪单帧位姿和筛选匹配的内点时使用了，这个是使用CUDA流并行的，每一个帧对开一个流，然后在每一个流中使用多个线程并行。

2. 在局部BA 下采样的时候使用CUDA_cache调用CUDA加速了。
3. 优化的时候使用CUDASloverBundling加速了，这个我还没有看明白是什么加速的。

## 插入关键帧的时机

第一帧作为关键帧，后面每一个跟踪成功和SBA优化之后的帧都作为关键帧

## 最耗时的地方

2.14中选择用于局部BA的关键帧、getFeatureMatchPairs和局部帧find_corres三个步骤，非常耗时。==认为是计算共视分数和特征匹配时间太长==

有几个思路：

1.把Loftr换成更快的特征匹配
2.看懂优化过程中需要的变量，然后避免使用每两个帧匹配的方法，通过引入地图点的对象，只做一个匹配，就能够获得对应关系。必须要引入地图点对象，不然太慢了

3.把他的后端放orbslam2中

4.把选择用于局部BA的关键帧、getFeatureMatchPairs两个步骤，直接使用最近的10个关键帧代替，不进行共视图权重的筛选，考虑到卫星都是匀速转动的。





## 要学会的地方

CUDA加速

Instant NGP的加速思想

一个以对象为中心的NeRF-SLAM框架